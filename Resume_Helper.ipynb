{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "d4A4kGJr0z3i",
        "PBD-9OsNowe4"
      ],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d0db6680b77349f08f4a5c0da5721141": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_25b5dfb998554c65bf45be95982c9da2",
              "IPY_MODEL_3f565f29c0a344989f6095eb3cfd902d",
              "IPY_MODEL_80034cf08dad4ce2be79ec4d43eb1cf5"
            ],
            "layout": "IPY_MODEL_d850f8ac8f664296a4949fc12c27702e"
          }
        },
        "25b5dfb998554c65bf45be95982c9da2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d35fa686ea643c8b11f0ade4448cdb2",
            "placeholder": "​",
            "style": "IPY_MODEL_c76962046538498dbdafa0b3285b3a89",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3f565f29c0a344989f6095eb3cfd902d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84339c6908084ab5b65e5225ef2dd2dd",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ed7f897d81e4c18af59a395de3e7bdd",
            "value": 2
          }
        },
        "80034cf08dad4ce2be79ec4d43eb1cf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b131ce973364368a505510dfe91c17d",
            "placeholder": "​",
            "style": "IPY_MODEL_d25f9ef66ce5400cb94dd32ed2c7405a",
            "value": " 2/2 [00:14&lt;00:00,  6.19s/it]"
          }
        },
        "d850f8ac8f664296a4949fc12c27702e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d35fa686ea643c8b11f0ade4448cdb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c76962046538498dbdafa0b3285b3a89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84339c6908084ab5b65e5225ef2dd2dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ed7f897d81e4c18af59a395de3e7bdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b131ce973364368a505510dfe91c17d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d25f9ef66ce5400cb94dd32ed2c7405a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6878c758e204a16a505a876b4587598": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_17e62af37b9841d0b68167d3efa584be",
              "IPY_MODEL_16014c096064434d9bf251131816e9a3",
              "IPY_MODEL_e8b3330f42e24e779ed25f147cb72c4e"
            ],
            "layout": "IPY_MODEL_616aa9631ae94153aeb7cb5ec29405b2"
          }
        },
        "17e62af37b9841d0b68167d3efa584be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50f10af4d9894d658f8f1e4d4cd7b102",
            "placeholder": "​",
            "style": "IPY_MODEL_c80967e2185b439593ceb7ca5899b11b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "16014c096064434d9bf251131816e9a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_061ebea6a0244838a4d165f6aa7df455",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b5deb156ff343eab2e00ba269e4fd43",
            "value": 2
          }
        },
        "e8b3330f42e24e779ed25f147cb72c4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_417e1f29515c408cb1c9129a481a4b5d",
            "placeholder": "​",
            "style": "IPY_MODEL_5baa0dcf687340cdba9728506c35f6b9",
            "value": " 2/2 [00:21&lt;00:00,  8.81s/it]"
          }
        },
        "616aa9631ae94153aeb7cb5ec29405b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50f10af4d9894d658f8f1e4d4cd7b102": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c80967e2185b439593ceb7ca5899b11b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "061ebea6a0244838a4d165f6aa7df455": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b5deb156ff343eab2e00ba269e4fd43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "417e1f29515c408cb1c9129a481a4b5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5baa0dcf687340cdba9728506c35f6b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4a22c0e8c474a1a9cfd18043c55658e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f967b21c8a5d4469a512f2985b399bb3",
              "IPY_MODEL_754b8d4a3b464432b28ac6573aa01144",
              "IPY_MODEL_218f11a5d051445e9fcd24084ac172c8"
            ],
            "layout": "IPY_MODEL_d1e1dd1022644861ae5b37350eeb81a9"
          }
        },
        "f967b21c8a5d4469a512f2985b399bb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bf7e7c211624d8c8864b20abb38614d",
            "placeholder": "​",
            "style": "IPY_MODEL_67e479eb50764094a4077f44f3783692",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "754b8d4a3b464432b28ac6573aa01144": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58356c1160a2452fb9740bf7cc71436a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d40af31181b448a39c9236b9add12499",
            "value": 2
          }
        },
        "218f11a5d051445e9fcd24084ac172c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_367746f5229548cea0f904311f1de067",
            "placeholder": "​",
            "style": "IPY_MODEL_128d03c010ec47cfb05f66794554126c",
            "value": " 2/2 [00:17&lt;00:00,  7.35s/it]"
          }
        },
        "d1e1dd1022644861ae5b37350eeb81a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bf7e7c211624d8c8864b20abb38614d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67e479eb50764094a4077f44f3783692": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58356c1160a2452fb9740bf7cc71436a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d40af31181b448a39c9236b9add12499": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "367746f5229548cea0f904311f1de067": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "128d03c010ec47cfb05f66794554126c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "d4A4kGJr0z3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio duckduckgo-images-api huggingface_hub transformers gradio_pdf PyMuPDF torch -U bitsandbytes PyPDF2 pytesseract loguru bing_image_downloader"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GCHp-yaZcHE-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89b5d12a-4ee8-4167-e071-5805369c3411"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.25.2)\n",
            "Requirement already satisfied: duckduckgo-images-api in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: gradio_pdf in /usr/local/lib/python3.11/dist-packages (0.0.22)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.25.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: loguru in /usr/local/lib/python3.11/dist-packages (0.7.3)\n",
            "Requirement already satisfied: bing_image_downloader in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.8.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.5)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-images-api) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18.4->duckduckgo-images-api) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18.4->duckduckgo-images-api) (2.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig # used when quatizing models\n",
        "import torch\n",
        "from threading import Thread\n",
        "from transformers import TextIteratorStreamer"
      ],
      "metadata": {
        "id": "YisEPYNtfBPm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login # used to login into huggingFace\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM # to access llm models\n",
        "import gradio as gr # ui interface\n",
        "from PIL import Image # to open images\n",
        "import os # used to manipulate files/directories\n",
        "import time\n",
        "import uuid # used in giving unique names to each file to generate a unique file path\n",
        "import pymupdf # to open pdf document\n",
        "from gradio_pdf import PDF # to show pdf in the gradio interface\n",
        "from PyPDF2 import PdfReader # used to read the PDF\n",
        "from pytesseract import image_to_string # used to extract text from images"
      ],
      "metadata": {
        "id": "iVvzVgLP02am"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from pathlib import Path\n",
        "from loguru import logger # for logging errors\n",
        "from bing_image_downloader import downloader # to download images from the web\n",
        "import shutil # to delete the directory created immediately after serving its purpose"
      ],
      "metadata": {
        "id": "cboPei4fqalD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Signing into Hugging Face"
      ],
      "metadata": {
        "id": "ktTxNWc21ZIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "C4n42qU64Mnc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd2dfb25-5352-4083-97b7-9f5054b6d49f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: read).\n",
            "The token `HF_TOKEN` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `HF_TOKEN`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining models, Quantizing and creating Tokenizer for selected model"
      ],
      "metadata": {
        "id": "b0LVYAUhca8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mistral=\"mistralai/Mistral-7B-Instruct-v0.1\" # too heavy for runtime (Crashed CPU runtime)\n",
        "allenai=\"allenai/OLMoE-1B-7B-0125-Instruct\"\n",
        "llama=\"meta-llama/Llama-3.3-70B-Instruct\"\n",
        "qwenai72B=\"Qwen/Qwen2.5-72B-Instruct\"\n",
        "qwenai7B = \"Qwen/Qwen2-7B-Instruct\"\n",
        "deepseek=\"deepseek-ai/DeepSeek-R1\"\n",
        "GEMMA2 = \"google/gemma-2-2b-it\" # smallest model, fit well with compute size"
      ],
      "metadata": {
        "id": "eP1yvvqj2kYT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# performing quantization on model\n",
        "quant_config = BitsAndBytesConfig(\n",
        "  load_in_4bit=True,\n",
        "  bnb_4bit_use_double_quant=True,\n",
        "  #bnb_4bit_compute_dtype=torch.float32,  # Use a CPU-compatible dtype\n",
        "  bnb_4bit_compute_dtype=torch.bfloat16, # use if CUDA is available on system\n",
        "  bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# declaring tokenizer for chosen llm\n",
        "tokenizer = AutoTokenizer.from_pretrained(GEMMA2)"
      ],
      "metadata": {
        "id": "1zRI2VafDC7U"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display and Extracting Text From Files (PDF, .docx & images)"
      ],
      "metadata": {
        "id": "T9seMLX1cqpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_pdf(file_path):\n",
        "    \"\"\"\n",
        "    This function simply returns the file path for PDF display.\n",
        "    If it's a Word document, it converts it to PDF first.\n",
        "    \"\"\"\n",
        "    if file_path is None:\n",
        "        return None\n",
        "\n",
        "    # Check if file is a PDF or needs conversion\n",
        "    file_ext = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_ext == '.pdf':\n",
        "        # If already a PDF, just return the path\n",
        "        return file_path\n",
        "    elif file_ext == '.docx':\n",
        "        # If it's a Word document, convert to PDF\n",
        "        pdf_path = os.path.splitext(file_path)[0] + \"_converted.pdf\"\n",
        "\n",
        "        try:\n",
        "            with pymupdf.open(file_path) as doc:\n",
        "                pdf_bytes = doc.convert_to_pdf()\n",
        "\n",
        "                with open(pdf_path, 'wb') as pdf_file:\n",
        "                    pdf_file.write(pdf_bytes)\n",
        "\n",
        "            return pdf_path\n",
        "        except Exception as e:\n",
        "            print(f\"Error converting document to PDF: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        # Unsupported file type\n",
        "        print(f\"Unsupported file type: {file_ext}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "Jw3EdncIYtTX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(file_path):\n",
        "    pdf_reader = PdfReader(file_path)\n",
        "\n",
        "    raw_text = ''\n",
        "\n",
        "    for i, page in enumerate(pdf_reader.pages):\n",
        "\n",
        "        text = page.extract_text()\n",
        "        if text:\n",
        "            raw_text += text\n",
        "\n",
        "    return raw_text"
      ],
      "metadata": {
        "id": "LiT3_m6bCeAS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(file_path):\n",
        "    with pymupdf.open(file_path) as f:\n",
        "      if f.is_pdf:\n",
        "          return extract_text_from_pdf(file_path)\n",
        "      else:\n",
        "        pdf_bytes = f.convert_to_pdf()\n",
        "        unique_filename = f\"{uuid.uuid4()}.pdf\" # gives file a unique name\n",
        "\n",
        "        tmp_file_path = os.path.join(os.path.dirname(file_path), unique_filename) # creates a location for new file\n",
        "\n",
        "        try:\n",
        "            with open(tmp_file_path, 'wb') as tmp_file:\n",
        "                tmp_file.write(pdf_bytes)\n",
        "\n",
        "            text = extract_text_from_pdf(tmp_file_path)\n",
        "\n",
        "            return text\n",
        "        finally:\n",
        "            # Always clean up temporary file\n",
        "            if os.path.exists(tmp_file_path):\n",
        "                os.remove(tmp_file_path)"
      ],
      "metadata": {
        "id": "cg3ToQnbwtzL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text_with_pyPDF = extract_text(\"/content/Screenshot 2025-01-17 at 17.45.18.png\")\n",
        "#print(text_with_pyPDF)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gEPYxoud1DyS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_img(online_resume):\n",
        "    image_content = []\n",
        "    raw_text = str(image_to_string(online_resume))\n",
        "    image_content.append(raw_text)\n",
        "\n",
        "    return \"\\n\".join(image_content)"
      ],
      "metadata": {
        "id": "sUry-xoC6_3I"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text_with_pytesseract = extract_text_from_img(\"/content/Screenshot 2025-01-17 at 17.45.18.png\")\n",
        "#print(text_with_pytesseract)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nXyQW3EV9jqx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompting LLMs"
      ],
      "metadata": {
        "id": "jK5R08ixc4Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_pdf_prompt(text):\n",
        "  # extract text from pdf\n",
        "  #extracted_text = extract_text(pdf_file)\n",
        "\n",
        "  if not text:\n",
        "        return \"No text found in the PDF. Please upload a different file.\"\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  Here is the extracted text from the document, please go throgh it and help me improve it:\n",
        "  {text}\n",
        "  \"\"\"\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "atcmLwTb2fF2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_assistant_message(text):\n",
        "    parts = text.rsplit(\"Assistant:\", 1)\n",
        "    if len(parts) > 1:\n",
        "        return parts[1].strip()\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "pLIH3T08Hzkn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompting for User's PDF"
      ],
      "metadata": {
        "id": "g1vAniDvdBp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_user_resume(text):\n",
        "    # Generate prompt\n",
        "    user_prompt = generate_pdf_prompt(text)\n",
        "\n",
        "    system_prompt = \"\"\"\n",
        "    You are a helpful assistant that knows all about recruiting, you do the following:\n",
        "    - receive resume/cv document\n",
        "    - analyzes the contents in the resume and returns helpful feedback like:\n",
        "    mispelt words, better use of grammar, summarizing texts into bullet points\n",
        "    where needed, but not limited to these alone.\n",
        "    - provide useful feedback in form of bullet points, reference the original text from the document if needed.\n",
        "    - in your final feedback return the resume back to the user as it was in the markdown format with the changes made, indicate the places where the changes were made with a bracket at the end of the sentence like this (AI).\n",
        "    \"\"\"\n",
        "    gemma_prompt = system_prompt + \"\\n\\n\" + user_prompt\n",
        "    messages = [\n",
        "        #{\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": gemma_prompt}\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        # Apply chat template\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            return_tensors='pt',\n",
        "            add_generation_prompt=True,\n",
        "            return_dict=True,\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "\n",
        "        # Move to CUDA if available\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = inputs.to('cuda')\n",
        "\n",
        "    except Exception as e:\n",
        "        # Fallback approach\n",
        "        combined_text = f\"{system_prompt}\\n\\nUser: {user_prompt}\\n\\nAssistant:\"\n",
        "        inputs = tokenizer(combined_text, return_tensors='pt', padding=True)\n",
        "\n",
        "        # Move to CUDA if available\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = inputs.to('cuda')\n",
        "\n",
        "    # Load model\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        GEMMA2,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=quant_config\n",
        "    )\n",
        "\n",
        "    # Generate output\n",
        "    with torch.no_grad():  # Disable gradient tracking to save memory\n",
        "        output = model.generate(\n",
        "            input_ids=inputs.get('input_ids', inputs), # If 'input_ids' key exists, use it, otherwise use the whole inputs\n",
        "            max_new_tokens=1500,\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode output\n",
        "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    result = extract_assistant_message(decoded_output)\n",
        "\n",
        "    # Explicitly free memory\n",
        "    del model, inputs, output\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "jKWqj5fdO31_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(analyze_user_resume(\"/content/ODESANYA KONYINSOLA'S RESUME.pdf\"))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hmZFOtMP6sRx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Online Resumes"
      ],
      "metadata": {
        "id": "G1EptmTndMFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_resumes_from_web(company_name, job_type):\n",
        "    search_input = f\"{job_type} {company_name} resume template\"\n",
        "    base_output_dir = \"/content/online_resumes\"\n",
        "\n",
        "    # Create base directory\n",
        "    os.makedirs(base_output_dir, exist_ok=True)\n",
        "\n",
        "    online_resumes = []\n",
        "\n",
        "    try:\n",
        "        # Download images\n",
        "        downloaded_files = downloader.download(\n",
        "            search_input,\n",
        "            limit=3,\n",
        "            output_dir=base_output_dir,\n",
        "            adult_filter_off=True,\n",
        "            force_replace=False,\n",
        "            timeout=60\n",
        "        )\n",
        "\n",
        "        # The downloader creates a subfolder with the search query name\n",
        "        # We need to find this subfolder\n",
        "        expected_subfolder = os.path.join(base_output_dir, search_input)\n",
        "\n",
        "        # Check if the expected subfolder exists\n",
        "        if os.path.exists(expected_subfolder) and os.path.isdir(expected_subfolder):\n",
        "            print(f\"Found subfolder: {expected_subfolder}\")\n",
        "\n",
        "            # Get all image files in the subfolder\n",
        "            image_files = [os.path.join(expected_subfolder, f) for f in os.listdir(expected_subfolder)\n",
        "                          if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff'))]\n",
        "\n",
        "            print(f\"Found {len(image_files)} image files\")\n",
        "\n",
        "            # Process each image file\n",
        "            for i, file_path in enumerate(image_files):\n",
        "                try:\n",
        "                    print(f\"Processing image {i+1}: {file_path}\")\n",
        "\n",
        "                    # Open the image file\n",
        "                    img = Image.open(file_path)\n",
        "\n",
        "                    # Extract text from the image\n",
        "                    extracted_text = extract_text_from_img(img)\n",
        "\n",
        "                    if extracted_text:\n",
        "                        print(f\"Successfully extracted text from image {i+1}\")\n",
        "                        online_resumes.append(extracted_text)\n",
        "                    else:\n",
        "                        print(f\"No text extracted from image {i+1}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing image {i+1}: {e}\")\n",
        "        else:\n",
        "            print(f\"Expected subfolder not found: {expected_subfolder}\")\n",
        "\n",
        "            # Look for alternative subfolders\n",
        "            subfolders = [d for d in os.listdir(base_output_dir)\n",
        "                         if os.path.isdir(os.path.join(base_output_dir, d))]\n",
        "\n",
        "            if subfolders:\n",
        "                print(f\"Found alternative subfolders: {subfolders}\")\n",
        "\n",
        "                # Process the first alternative subfolder found\n",
        "                alt_folder = os.path.join(base_output_dir, subfolders[0])\n",
        "                image_files = [os.path.join(alt_folder, f) for f in os.listdir(alt_folder)\n",
        "                              if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff'))]\n",
        "\n",
        "                # Process images in alternative folder\n",
        "                for i, file_path in enumerate(image_files):\n",
        "                    try:\n",
        "                        img = Image.open(file_path)\n",
        "                        extracted_text = extract_text_from_img(img)\n",
        "\n",
        "                        if extracted_text:\n",
        "                            online_resumes.append(extracted_text)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing image in alternative folder: {e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during download process: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # Clean up - remove the base directory and all its contents\n",
        "        if os.path.exists(base_output_dir):\n",
        "            try:\n",
        "                shutil.rmtree(base_output_dir)\n",
        "                print(f\"Cleaned up temporary directory: {base_output_dir}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error cleaning up directory: {e}\")\n",
        "\n",
        "    print(f\"Total resumes extracted: {len(online_resumes)}\")\n",
        "    return online_resumes"
      ],
      "metadata": {
        "id": "4fRqrviODeeR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#online_resumes = get_resumes_from_web(\"Google\", \"Software Developer\")\n",
        "#print(online_resumes)"
      ],
      "metadata": {
        "id": "1TxnbcrppeNG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_online_resume_prompt(company_name, job_type, resume_text):\n",
        "    messages = [\n",
        "        #{\"role\": \"system\", \"content\": f\"You are a helpful assistant that knows all about recruiting for {company_name} for the job role of {job_type}, you have received a resume from the internet that are similar to the job the user wants to apply for.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"\"\"\n",
        "      You are a helpful assistant that knows all about recruiting for {company_name} for the job role of {job_type}, you have received a resume from the internet that are similar to the job the user wants to apply for.\n",
        "      You have received a resume from the internet that is similar to the job the user wants to apply for.\n",
        "      Your task is to:\n",
        "      - Gather important points/aspects used in the resume that will be useful to give insights to a user.\n",
        "      - Look for important keywords used relating to the '{job_type}' role.\n",
        "      - Return the response in a format that preserves the online resume as it will be used for comparison and similarity checking.\n",
        "\n",
        "      Here is the extracted text from the document:\n",
        "      {resume_text}\n",
        "      \"\"\"}\n",
        "          ]\n",
        "\n",
        "    return messages"
      ],
      "metadata": {
        "id": "Wkr5degyRXOW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompting for Online Resumes"
      ],
      "metadata": {
        "id": "TXgfah_TdS_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_online_resumes(company_name, job_type):\n",
        "    online_resumes = get_resumes_from_web(company_name, job_type)\n",
        "\n",
        "    if not online_resumes:\n",
        "        print(\"No valid resumes found to analyze\")\n",
        "        return []\n",
        "\n",
        "    insight = []\n",
        "\n",
        "    try:\n",
        "        # Load model once\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            GEMMA2,\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "            quantization_config=quant_config\n",
        "        )\n",
        "\n",
        "        for i, resume in enumerate(online_resumes):\n",
        "            if not resume:\n",
        "                print(f\"Skipping empty resume #{i+1}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                print(f\"Processing resume #{i+1}\")\n",
        "                messages = generate_online_resume_prompt(company_name, job_type, resume)\n",
        "\n",
        "                # Process inputs with proper error handling\n",
        "                try:\n",
        "                    inputs = tokenizer.apply_chat_template(\n",
        "                        messages,\n",
        "                        return_tensors='pt',\n",
        "                        add_generation_prompt=True,\n",
        "                        return_dict=True,\n",
        "                        return_attention_mask=True\n",
        "                    )\n",
        "\n",
        "                    if torch.cuda.is_available():\n",
        "                        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Chat template error for resume #{i+1}: {e}\")\n",
        "                    combined_text = f\"System: {messages[0]['content']}\\n\\nUser: {messages[1]['content']}\\n\\nAssistant:\"\n",
        "                    inputs = tokenizer(combined_text, return_tensors='pt', padding=True)\n",
        "\n",
        "                    if torch.cuda.is_available():\n",
        "                        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
        "\n",
        "                # Generate with torch.no_grad() to save memory\n",
        "                try:\n",
        "                    with torch.no_grad():\n",
        "                        output = model.generate(\n",
        "                            input_ids=inputs.get('input_ids', inputs), # If 'input_ids' key exists, use it, otherwise use the whole inputs\n",
        "                            max_new_tokens=1500,\n",
        "                            attention_mask=inputs['attention_mask'],\n",
        "                            pad_token_id=tokenizer.eos_token_id\n",
        "                        )\n",
        "\n",
        "                    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "                    processed_output = extract_assistant_message(decoded_output)\n",
        "\n",
        "                    if processed_output:\n",
        "                        insight.append(processed_output)\n",
        "                        print(f\"Successfully processed resume #{i+1}\")\n",
        "                    else:\n",
        "                        print(f\"No valid output generated for resume #{i+1}\")\n",
        "\n",
        "                    # Free memory after each resume processing\n",
        "                    del output, inputs, decoded_output\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Generation error for resume #{i+1}: {e}\")\n",
        "                    # Free any partial resources\n",
        "                    if 'inputs' in locals():\n",
        "                        del inputs\n",
        "                    if 'output' in locals():\n",
        "                        del output\n",
        "                    torch.cuda.empty_cache()\n",
        "                    continue\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Processing error for resume #{i+1}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Free model memory after all processing is complete\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Model loading error: {e}\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Successfully analyzed {len(insight)} out of {len(online_resumes)} resumes\")\n",
        "    return insight"
      ],
      "metadata": {
        "id": "qx9egF2PRkW_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(analyze_online_resumes(\"Google\", \"Software Developer\"))"
      ],
      "metadata": {
        "id": "Nuus-CTb0Jyy",
        "collapsed": true
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Prompting -- Combining Both User and Online Resume Insight"
      ],
      "metadata": {
        "id": "mlQq5L7IdX17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_resume_insight_prompt(user_resume_insight, formatted_online_insights):\n",
        "    prompt = f\"\"\"\n",
        "      Here is the insight from the user's resume:\n",
        "      {user_resume_insight}\n",
        "\n",
        "      Here are the insights from the online resumes:\n",
        "      {formatted_online_insights}\n",
        "\n",
        "      Tailor the resume of the user based on information gotten from online resumes.\n",
        "      \"\"\"\n",
        "\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "RDyobV5DUic1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_resume_insight(pdf_file, company_name, job_type):\n",
        "    user_resume_insight = analyze_user_resume(pdf_file)\n",
        "    online_resume_insight = analyze_online_resumes(company_name, job_type)\n",
        "\n",
        "    formatted_online_insights = \"\\n\\n\".join(online_resume_insight)\n",
        "\n",
        "    system_prompt = f\"\"\"\n",
        "    You are a helpful assistant that knows all about recruiting for {company_name} for the job role of {job_type}.\n",
        "    You have received insights from the user's resume and multiple insights from online resumes.\n",
        "\n",
        "    You are to:\n",
        "    - Compare and gather similarities from all the online resumes.\n",
        "    - Use that to improve the user's resume for the '{job_type}' role.\n",
        "    - Do NOT add any activity/project/experience that is not already present in the user's resume.\n",
        "    - In your final feedback return the resume back to the user as it was in the markdown format with the changes made, indicate the places where the changes were made with an asterick or so.\n",
        "    Provide the result in **markdown format**.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get user prompt\n",
        "    user_prompt = generate_resume_insight_prompt(user_resume_insight, formatted_online_insights)\n",
        "\n",
        "    # Free memory from previous steps\n",
        "    if 'user_resume_insight' in locals():\n",
        "        del user_resume_insight\n",
        "    if 'online_resume_insight' in locals():\n",
        "        del online_resume_insight\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    gemma_prompt = system_prompt + \"\\n\\n\" + user_prompt\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": gemma_prompt}\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        # Load model\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            GEMMA2,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=quant_config\n",
        "        )\n",
        "\n",
        "        # Apply chat template\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            return_tensors=\"pt\",\n",
        "            add_generation_prompt=True,\n",
        "            return_dict=True,\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "\n",
        "        # Move to CUDA if available\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = inputs.to(\"cuda\")\n",
        "\n",
        "        # Setup streamer\n",
        "        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "        # Generate in a separate thread to enable streaming\n",
        "        generation_kwargs = {\n",
        "            \"input_ids\": inputs.get('input_ids', inputs),\n",
        "            \"attention_mask\": inputs.get('attention_mask', None),\n",
        "            \"max_new_tokens\": 1500,\n",
        "            \"streamer\": streamer,\n",
        "            \"pad_token_id\": tokenizer.eos_token_id\n",
        "        }\n",
        "\n",
        "        thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
        "        thread.start()\n",
        "\n",
        "        # Yield output for streaming in Gradio\n",
        "        generated_text = \"\"\n",
        "        for text_chunk in streamer:\n",
        "            generated_text += text_chunk\n",
        "            # This is the key part for Gradio streaming\n",
        "            yield generated_text\n",
        "\n",
        "        thread.join()\n",
        "\n",
        "        # Free memory\n",
        "        del model, inputs, streamer\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"An error occurred while processing the resume: {str(e)}\"\n",
        "        print(f\"Error in get_resume_insight: {e}\")\n",
        "        # Clean up in case of error\n",
        "        if 'model' in locals():\n",
        "            del model\n",
        "        if 'inputs' in locals():\n",
        "            del inputs\n",
        "        if 'streamer' in locals():\n",
        "            del streamer\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        yield error_msg"
      ],
      "metadata": {
        "id": "VJy0wTzorxym"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_resume_insight_interface(file_path, company_name, job_type):\n",
        "    resume_text = extract_text(file_path)  # Extract raw text from file\n",
        "    return get_resume_insight(resume_text, company_name, job_type)  # Call your core logic"
      ],
      "metadata": {
        "id": "lUv9B05mYMLv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio Interface"
      ],
      "metadata": {
        "id": "Xh6VLZEcQRu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as ui:\n",
        "  gr.Markdown(\"# Upload Your Resume/CV and get valuable insight on how to improve it 😉\")\n",
        "  with gr.Row():\n",
        "    with gr.Column(variant='panel', scale=5):\n",
        "      file = gr.File(label=\"Please upload a PDF or Word Document of your resume\", file_types=[\".pdf\", \".docx\"])\n",
        "      company_name = gr.Textbox(label = \"Input name of company intrested in applying to\", placeholder = \"e.g. Google, Amazon, KPMG, ARCO\", value = \"Google\")\n",
        "      job_role = gr.Textbox(label = \"Input intrested job role\", placeholder = \"e.g. Software Developer, Accountant, Business Analyst\", value = \"Software Developer\")\n",
        "      with gr.Row():\n",
        "        generate_btn = gr.Button(\"Generate\")\n",
        "        clear_btn = gr.ClearButton(value=\"Clear\")\n",
        "      show_pdf = PDF(label='Document preview', interactive=False, visible=True, height=800)\n",
        "\n",
        "    with gr.Column(variant='panel', scale=5):\n",
        "      output = gr.Markdown(label=\"Generated Output\", height=1100, show_copy_button=True, line_breaks=True)\n",
        "\n",
        "  file.change(fn=display_pdf, inputs=file, outputs=show_pdf)\n",
        "  generate_btn.click(fn=get_resume_insight_interface, inputs=[file, company_name, job_role], outputs=output)\n",
        "  clear_btn.add([file, company_name, job_role, output, show_pdf])\n",
        "\n",
        "ui.launch(inbrowser=True, debug=True)"
      ],
      "metadata": {
        "id": "yJID3kMTQU33",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d0db6680b77349f08f4a5c0da5721141",
            "25b5dfb998554c65bf45be95982c9da2",
            "3f565f29c0a344989f6095eb3cfd902d",
            "80034cf08dad4ce2be79ec4d43eb1cf5",
            "d850f8ac8f664296a4949fc12c27702e",
            "5d35fa686ea643c8b11f0ade4448cdb2",
            "c76962046538498dbdafa0b3285b3a89",
            "84339c6908084ab5b65e5225ef2dd2dd",
            "1ed7f897d81e4c18af59a395de3e7bdd",
            "3b131ce973364368a505510dfe91c17d",
            "d25f9ef66ce5400cb94dd32ed2c7405a",
            "f6878c758e204a16a505a876b4587598",
            "17e62af37b9841d0b68167d3efa584be",
            "16014c096064434d9bf251131816e9a3",
            "e8b3330f42e24e779ed25f147cb72c4e",
            "616aa9631ae94153aeb7cb5ec29405b2",
            "50f10af4d9894d658f8f1e4d4cd7b102",
            "c80967e2185b439593ceb7ca5899b11b",
            "061ebea6a0244838a4d165f6aa7df455",
            "3b5deb156ff343eab2e00ba269e4fd43",
            "417e1f29515c408cb1c9129a481a4b5d",
            "5baa0dcf687340cdba9728506c35f6b9",
            "c4a22c0e8c474a1a9cfd18043c55658e",
            "f967b21c8a5d4469a512f2985b399bb3",
            "754b8d4a3b464432b28ac6573aa01144",
            "218f11a5d051445e9fcd24084ac172c8",
            "d1e1dd1022644861ae5b37350eeb81a9",
            "7bf7e7c211624d8c8864b20abb38614d",
            "67e479eb50764094a4077f44f3783692",
            "58356c1160a2452fb9740bf7cc71436a",
            "d40af31181b448a39c9236b9add12499",
            "367746f5229548cea0f904311f1de067",
            "128d03c010ec47cfb05f66794554126c"
          ]
        },
        "outputId": "f3a875fc-c0a9-468d-d8fc-84e17ff619ac"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://359550666c2fd18a87.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://359550666c2fd18a87.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0db6680b77349f08f4a5c0da5721141"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[%] Downloading Images to /content/online_resumes/Software Developer Google resume template\n",
            "\n",
            "\n",
            "[!!]Indexing page: 1\n",
            "\n",
            "[%] Indexed 3 Images on Page 1.\n",
            "\n",
            "===============================================\n",
            "\n",
            "[%] Downloading Image #1 from https://www.hloom.com/images/Boast-gdoc.jpg\n",
            "[%] File Downloaded !\n",
            "\n",
            "[%] Downloading Image #2 from https://cdn.resumecatstatic.com/resume-examples/google-software-developer-resume-example-OPEfo-1600w.png\n",
            "[%] File Downloaded !\n",
            "\n",
            "[%] Downloading Image #3 from https://as2.ftcdn.net/v2/jpg/05/75/51/51/1000_F_575515196_ljyVLoydCv5ovTDS1n22PuRqar4xERj6.jpg\n",
            "[%] File Downloaded !\n",
            "\n",
            "\n",
            "\n",
            "[%] Done. Downloaded 3 images.\n",
            "Found subfolder: /content/online_resumes/Software Developer Google resume template\n",
            "Found 3 image files\n",
            "Processing image 1: /content/online_resumes/Software Developer Google resume template/Image_2.png\n",
            "Successfully extracted text from image 1\n",
            "Processing image 2: /content/online_resumes/Software Developer Google resume template/Image_1.jpg\n",
            "Successfully extracted text from image 2\n",
            "Processing image 3: /content/online_resumes/Software Developer Google resume template/Image_3.jpg\n",
            "Successfully extracted text from image 3\n",
            "Cleaned up temporary directory: /content/online_resumes\n",
            "Total resumes extracted: 3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6878c758e204a16a505a876b4587598"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing resume #1\n",
            "Successfully processed resume #1\n",
            "Processing resume #2\n",
            "Successfully processed resume #2\n",
            "Processing resume #3\n",
            "Successfully processed resume #3\n",
            "Successfully analyzed 3 out of 3 resumes\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4a22c0e8c474a1a9cfd18043c55658e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You're right! I apologize for the previous response. I was focusing on the general structure of resumes and not on the specific details needed for a Software Developer role. \n",
            "\n",
            "Here's a breakdown of the user's resume and how to improve it, along with some additional tips for a successful application:\n",
            "\n",
            "**Analysis of the User's Resume:**\n",
            "\n",
            "* **Missing Information:**  The resume lacks crucial details like:\n",
            "    * **Full Name:**  This is essential for professional communication.\n",
            "    * **Contact Information:**  Phone number, email address, and LinkedIn profile are crucial.\n",
            "    * **Education:**  Degree, university, major, and relevant coursework.\n",
            "    * **Technical Skills:**  Programming languages, frameworks, tools, and technologies.\n",
            "    * **Projects:**  Personal or academic projects that demonstrate skills.\n",
            "    * **Experience:**  Specific job titles, responsibilities, and accomplishments.\n",
            "* **Generic Language:**  The resume uses vague terms and lacks specific examples.\n",
            "* **No Keywords:**  The resume doesn't use keywords relevant to a Software Developer role.\n",
            "\n",
            "\n",
            "**How to Improve the Resume:**\n",
            "\n",
            "1. **Contact Information:**  Ensure the user's full name, phone number, email address, and LinkedIn profile are clearly listed.\n",
            "2. **Education:**  Provide the full name of the university, degree, major, and any relevant coursework.\n",
            "3. **Technical Skills:**  List programming languages, frameworks, tools, and technologies used. Be specific and use industry-standard terminology.\n",
            "4. **Projects:**  Describe personal or academic projects in detail, highlighting the technologies used, the problem solved, and the results achieved.\n",
            "5. **Experience:**  Provide specific job titles, responsibilities, and accomplishments. Use the STAR method (Situation, Task, Action, Result) to describe achievements. Quantify results whenever possible.\n",
            "6. **Keywords:**  Research keywords used in job descriptions for Software Developer roles and incorporate them into the resume.\n",
            "7. **Formatting:**  Use a professional resume template and ensure proper formatting. \n",
            "8. **Proofread:**  Carefully proofread the resume for any errors in grammar, spelling, and punctuation.\n",
            "\n",
            "**Additional Tips for a Successful Application:**\n",
            "\n",
            "* **Tailor the Resume:**  Customize the resume for each job application, highlighting the skills and experience most relevant to the specific role.\n",
            "* **Cover Letter:**  Write a compelling cover letter that explains your interest in the company and the position.\n",
            "* **Portfolio:**  Create a portfolio website or GitHub profile to showcase your projects and skills.\n",
            "* **Network:**  Attend industry events and connect with professionals in the field.\n",
            "* **Practice Interviewing:**  Prepare for technical and behavioral interview questions. \n",
            "* **Research Google:**  Learn about Google's culture, values, and the specific team you're applying to. \n",
            "\n",
            "**Remember:**  A strong resume is just one part of the application process.  Be prepared to answer questions, demonstrate your skills, and showcase your passion for software development. \n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://359550666c2fd18a87.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FALLBACK CODE"
      ],
      "metadata": {
        "id": "PBD-9OsNowe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_resume_insight_old(pdf_file, company_name, job_type):\n",
        "    user_resume_insight = analyze_user_resume(pdf_file)\n",
        "    online_resume_insight = analyze_online_resumes(company_name, job_type)\n",
        "\n",
        "    formatted_online_insights = \"\\n\\n\".join(online_resume_insight)\n",
        "\n",
        "    system_prompt = f\"\"\"\n",
        "    You are a helpful assistant that knows all about recruiting for {company_name} for the job role of {job_type}.\n",
        "    You have received insights from the user's resume and multiple insights from online resumes.\n",
        "\n",
        "    You are to:\n",
        "    - Compare and gather similarities from all the online resumes.\n",
        "    - Use that to improve the user's resume for the '{job_type}' role.\n",
        "    - Do NOT add any activity/project/experience that is not already present in the user's resume.\n",
        "    - In your final feedback return the resume back to the user as it was in the markdown format with the changes made, indicate the places where the changes were made with an asterick or so.\n",
        "\n",
        "    Provide the result in **markdown format**.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get user prompt\n",
        "    user_prompt = generate_resume_insight_prompt(user_resume_insight, formatted_online_insights)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "\n",
        "    # Tokenize with attention mask\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        return_tensors='pt',\n",
        "        return_attention_mask=True\n",
        "    ).to('cuda')\n",
        "\n",
        "    # Load model\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        mistral,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=quant_config\n",
        "    )\n",
        "\n",
        "    # Prepare streamer\n",
        "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    generation_kwargs = {\n",
        "        \"input_ids\": inputs[\"input_ids\"],\n",
        "        \"attention_mask\": inputs[\"attention_mask\"],  # pass attention_mask\n",
        "        \"max_new_tokens\": 1500,\n",
        "        \"streamer\": streamer,\n",
        "        \"pad_token_id\": tokenizer.eos_token_id  # to avoids warnings\n",
        "    }\n",
        "\n",
        "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
        "    thread.start()\n",
        "\n",
        "    full_output = \"\"\n",
        "    for text_chunk in streamer:\n",
        "        print(text_chunk, end=\"\", flush=True)\n",
        "        full_output += text_chunk\n",
        "\n",
        "    thread.join()\n",
        "    return full_output"
      ],
      "metadata": {
        "id": "_uknettQovdE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_resumes_from_web_old(company_name, job_type):\n",
        "  search_input = f\"{job_type} {company_name} resume template\"\n",
        "  results = search(search_input, max_results=3)\n",
        "  print(f\"--RESULTS-- {results}\")\n",
        "  if not results:\n",
        "    print(\"No images found.\")\n",
        "    return []\n",
        "\n",
        "  online_resumes = []\n",
        "  for i, result in enumerate(results):\n",
        "    print(f\"FOR LOOP RESULT -- {result}\")\n",
        "    print(f\"\\n--- Image {i+1} ---\")\n",
        "    image_url = result['results'][i]['image']\n",
        "    print(f\"Image URL: {image_url}\")\n",
        "\n",
        "    try:\n",
        "      # Download and open image\n",
        "      response = requests.get(image_url)\n",
        "      img = Image.open(BytesIO(response.content)) # creates an in-memory binary stream to prevent writing to disk\n",
        "\n",
        "      # Converting the downloaded images into markdown and inserting them into a list\n",
        "      online_resumes.append(extract_text_from_img(img))\n",
        "    except Exception as e:\n",
        "      print(f\"Error processing image {i+1}: {e}\")\n",
        "      # exception to handle bad processing of an image\n",
        "      online_resumes.append(None)\n",
        "  return online_resumes"
      ],
      "metadata": {
        "id": "32-Eaxf4KhNX"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_online_resumes_old(company_name, job_type):\n",
        "    online_resumes = get_resumes_from_web(company_name, job_type)\n",
        "\n",
        "    # model running outside loop so as to conserve space and GPU processing\n",
        "    model = AutoModelForCausalLM.from_pretrained(mistral, device_map=\"auto\", quantization_config=quant_config)\n",
        "\n",
        "    insight = []\n",
        "    for resume in online_resumes:\n",
        "        if not resume:\n",
        "            continue  # Skip if empty, but continue with others\n",
        "\n",
        "        messages = generate_online_resume_prompt(company_name, job_type, resume)\n",
        "\n",
        "        input = tokenizer.apply_chat_template(messages, return_tensors='pt', return_attention_mask=True).to('cuda')\n",
        "\n",
        "        output = model.generate(\n",
        "            input_ids=input[\"input_ids\"],  # <-- pass input_ids explicitly\n",
        "            attention_mask=input[\"attention_mask\"],  # <-- pass attention_mask\n",
        "            max_new_tokens=1500,\n",
        "            pad_token_id=tokenizer.eos_token_id  # <-- to avoid warnings\n",
        "        )\n",
        "        insight.append(extract_after_last_inst(tokenizer.decode(output[0])))\n",
        "\n",
        "    return insight"
      ],
      "metadata": {
        "id": "IW68A934SXwI"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_user_resume_old(text):\n",
        "    # Generate string prompt\n",
        "    user_prompt = generate_pdf_prompt(text)\n",
        "\n",
        "    system_prompt = \"\"\"\n",
        "    You are a helpful assistant that knows all about recruiting, you do the following:\n",
        "    - receive resume/cv document\n",
        "    - analyzes the contents in the resume and returns helpful feedback like:\n",
        "    mispelt words, better use of grammar, summarizing texts into bullet points\n",
        "    where needed, but not limited to these alone.\n",
        "    - provide useful feedback in form of bullet points, reference the original text from the document if needed.\n",
        "    - in your final feedback return the resume back to the user as it was in the markdown format with the changes made, indicate the places where the changes were made with a bracket at the end of the sentence like this (AI).\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "\n",
        "    # Correctly handle the tokenizer output\n",
        "    # First check if tokenizer supports chat templates\n",
        "    try:\n",
        "        # Get the input tensors - most modern tokenizers return a dictionary\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            return_tensors='pt',\n",
        "            add_generation_prompt=True,\n",
        "            return_dict=True  # Explicitly request a dictionary\n",
        "        )\n",
        "\n",
        "        # Move to CUDA if available\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
        "\n",
        "    except Exception as e:\n",
        "        # Fallback approach if the chat template method fails\n",
        "        combined_text = f\"{system_prompt}\\n\\nUser: {user_prompt}\\n\\nAssistant:\"\n",
        "        inputs = tokenizer(combined_text, return_tensors='pt', padding=True)\n",
        "\n",
        "        # Move to CUDA if available\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
        "\n",
        "    # Load model (only once)\n",
        "    model = AutoModelForCausalLM.from_pretrained(GEMMA2, device_map=\"auto\", quantization_config=quant_config)\n",
        "    #model = AutoModelForCausalLM.from_pretrained(GEMMA2, device_map=\"auto\")\n",
        "\n",
        "    # Generate output\n",
        "    output = model.generate(\n",
        "        **inputs,  # Pass all inputs at once\n",
        "        max_new_tokens=1500,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Decode and return only the generated portion\n",
        "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # This is model/tokenizer dependent\n",
        "    return extract_assistant_message(decoded_output)"
      ],
      "metadata": {
        "id": "zjp2k1dMoz6v"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}